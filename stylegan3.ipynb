{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harikang/harikang/blob/main/stylegan3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3iyFBFEVoeq"
      },
      "source": [
        "# 필요한 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7guwAKeZMNI",
        "outputId": "a3f892be-9f66-4136-8d99-05200f8d42d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMrswsoGeJ7A",
        "outputId": "1bbbc196-ec66-4950-8596-81c310c5f699"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/stylegan2-ada-pytorch')"
      ],
      "metadata": {
        "id": "z-0Upb-otq72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Dt18De-OQRQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import copy\n",
        "import os\n",
        "from time import perf_counter\n",
        "import click\n",
        "import imageio\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import dnnlib\n",
        "import legacy\n",
        "import os\n",
        "import re\n",
        "from typing import List\n",
        "import click"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQFSFYiVVrG0"
      },
      "source": [
        "# 드라이브 마운트 및 경로 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0fCuAB0iCHJ",
        "outputId": "72f51315-2e90-4c62-9db9-b54e75bae5ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan3'...\n",
            "remote: Enumerating objects: 212, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 212 (delta 0), reused 1 (delta 0), pack-reused 207\u001b[K\n",
            "Receiving objects: 100% (212/212), 4.17 MiB | 1.53 MiB/s, done.\n",
            "Resolving deltas: 100% (99/99), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/NVlabs/stylegan3.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdMA_fuui_A_"
      },
      "outputs": [],
      "source": [
        "!export PYTHONPATH=."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IKTQL6Xsi7Lp",
        "outputId": "76393def-b419-42a6-fe87-29b58b421dc5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/stylegan2-ada-pytorch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6byS57QNitW0",
        "outputId": "bcec02f8-0ef8-4190-93f2-235ec6c5abfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ninja-linux.zip\n",
            "replace //content/drive/MyDrive/stylegan2-ada-pytorch/ninja? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: //content/drive/MyDrive/stylegan2-ada-pytorch/ninja  \n",
            "update-alternatives: priority must be an integer\n",
            "\n",
            "Use 'update-alternatives --help' for program usage information.\n"
          ]
        }
      ],
      "source": [
        "#error 방지를 위해 ninja를 설치 (ninja가 뭔진 모름)\n",
        "# !wget https://github.com/ninja-build/ninja/releases/download/v1.10.2/ninja-linux.zip #이 코드는 한번만 실행\n",
        "!sudo unzip ninja-linux.zip -d //content/drive/MyDrive/stylegan2-ada-pytorch\n",
        "!sudo update-alternatives --install //content/drive/MyDrive/stylegan2-ada-pytorch ninja //content/drive/MyDrive/stylegan2-ada-pytorch/ --force"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI-660qJrwjX",
        "outputId": "227bdb1e-c0fd-4784-a211-c1380a1b6c6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n"
          ]
        }
      ],
      "source": [
        "#error 방지를 위해 ninja를 설치 (ninja가 뭔진 모름)\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "# !watch -d -n 0.5 free -m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvJwdeaqi48m"
      },
      "outputs": [],
      "source": [
        "#결과 저장을 위한 디렉토리 생성\n",
        "# !mkdir training-runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahR8PN7LjR9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbb22a4-9fdb-4806-a3be-799237e5731f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 1908/1908 [01:07<00:00, 28.41it/s]\n"
          ]
        }
      ],
      "source": [
        "#train에 사용할 수 있는 dataset.zip 형태로 변경하는 py파일 돌림.\n",
        "# !python dataset_tool.py  --source /content/drive/MyDrive/stylegan3/closed_eyes  --dest /content/drive/MyDrive/stylegan3/training-runs/dataset.zip  --transform=center-crop-wide --resolution=512x512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7IzPMLoOAU6U"
      },
      "outputs": [],
      "source": [
        "# @title 후보 train 방법\n",
        "# # Train StyleGAN3-T for AFHQv2 using 8 GPUs.\n",
        "# python train.py --outdir=~/training-runs --cfg=stylegan3-t --data=~/datasets/afhqv2-512x512.zip \\\n",
        "#     --gpus=8 --batch=32 --gamma=8.2 --mirror=1\n",
        "\n",
        "# # Fine-tune StyleGAN3-R for MetFaces-U using 1 GPU, starting from the pre-trained FFHQ-U pickle.\n",
        "# python train.py --outdir=~/training-runs --cfg=stylegan3-r --data=~/datasets/metfacesu-1024x1024.zip \\\n",
        "#     --gpus=8 --batch=32 --gamma=6.6 --mirror=1 --kimg=5000 --snap=5 \\\n",
        "#     --resume=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-1024x1024.pkl\n",
        "\n",
        "# # Train StyleGAN2 for FFHQ at 1024x1024 resolution using 8 GPUs.\n",
        "# python train.py --outdir=~/training-runs --cfg=stylegan2 --data=~/datasets/ffhq-1024x1024.zip \\\n",
        "#     --gpus=8 --batch=32 --gamma=10 --mirror=1 --aug=noaug\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NuLh2EWZCBUL"
      },
      "outputs": [],
      "source": [
        "# @title 후보 train 예시\n",
        "# !python train.py --outdir=/content/drive/MyDrive/stylegan3/training-runs --cfg=stylegan3-r --data=/content/drive/MyDrive/stylegan3/training-runs/dataset.zip  --gpus=1 --batch=4 --gamma=6.6 --mirror=1 --kimg=1000 --snap=5 --resume=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-1024x1024.pkl --workers 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7Cgd7ESVMA6"
      },
      "source": [
        "python 명령어로 train 진행 (일단 5000개의 이미지만 돌림)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YcHMT17lIREQ",
        "outputId": "fc8c293d-18f7-4245-8904-24a3f21d403c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/stylegan2-ada-pytorch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 339
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNq-tKDJutYy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "# !python train.py --outdir=/content/drive/MyDrive/stylegan3/training-runs --cfg=stylegan3-t --data=/content/drive/MyDrive/stylegan3/training-runs/dataset.zip --gpus=1 --batch=4 --gamma=8.2 --mirror=1 --kimg=1 --snap=5  --workers 1\n",
        "# !python train.py --outdir=/content/drive/MyDrive/stylegan3/training-runs --data=/content/drive/MyDrive/stylegan3/training-runs/dataset.zip --gpus=1 --batch=4 --gamma=8.2 --mirror=1 --kimg=1 --snap=5  --workers 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn5Fn0m1HKib"
      },
      "source": [
        "Ultrasound 2 Latent vector (초음파 이미지를 z vector로 변경)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4T1yaa7HJ-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7bb2c12-6a24-4fbb-d741-4ee51a130c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 14.17 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "btUnCRSKV_qy",
        "outputId": "3deaf4ad-6005-42c0-e61c-685d62345b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/stylegan3/stylegan2-ada-pytorch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhY7vsC1Wra8"
      },
      "outputs": [],
      "source": [
        "#생성될 결과를 저장할 디렉토리 생성"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r3Eofv12HPz_",
        "outputId": "a851c58c-340b-47c5-a780-d070be4e7def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/stylegan2-ada-pytorch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA8FQGdDP7jr"
      },
      "outputs": [],
      "source": [
        "#Project given image to the latent space of pretrained network pickle. ->python 명령어로 실행하는 방법\n",
        "# python projector.py --outdir=out --target=~/mytargetimg.png --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mnmN64mV5EL"
      },
      "source": [
        "# 초음파 이미지를 latent vector로 project하는 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUMdhc_9WSdO"
      },
      "outputs": [],
      "source": [
        "def project(\n",
        "    G,\n",
        "    target: torch.Tensor, # [C,H,W] and dynamic range [0,255], W & H must match G output resolution\n",
        "    *,\n",
        "    num_steps                  = 1000,\n",
        "    w_avg_samples              = 1000,\n",
        "    initial_learning_rate      = 0.1,\n",
        "    initial_noise_factor       = 0.05,\n",
        "    lr_rampdown_length         = 0.25,\n",
        "    lr_rampup_length           = 0.05,\n",
        "    noise_ramp_length          = 0.75,\n",
        "    regularize_noise_weight    = 1e5,\n",
        "    verbose                    = False,\n",
        "    device: torch.device\n",
        "):\n",
        "    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution)\n",
        "\n",
        "    def logprint(*args):\n",
        "        if verbose:\n",
        "            print(*args)\n",
        "\n",
        "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device) # type: ignore\n",
        "\n",
        "    # Compute w stats.\n",
        "    logprint(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n",
        "    z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n",
        "    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
        "    w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)       # [N, 1, C]\n",
        "    w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 1, C]\n",
        "    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
        "\n",
        "    # Setup noise inputs.\n",
        "    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n",
        "\n",
        "    # Load VGG16 feature detector.\n",
        "    url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
        "    with dnnlib.util.open_url(url) as f:\n",
        "        vgg16 = torch.jit.load(f).eval().to(device)\n",
        "        print(vgg16)\n",
        "\n",
        "    # Features for target image.\n",
        "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
        "    if target_images.shape[2] > 256:\n",
        "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
        "    target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n",
        "\n",
        "    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n",
        "    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n",
        "    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
        "\n",
        "    # Init noise.\n",
        "    for buf in noise_bufs.values():\n",
        "        buf[:] = torch.randn_like(buf)\n",
        "        buf.requires_grad = True\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        # Learning rate schedule.\n",
        "        t = step / num_steps\n",
        "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
        "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
        "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
        "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
        "        lr = initial_learning_rate * lr_ramp\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # Synth images from opt_w.\n",
        "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
        "        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n",
        "        synth_images = G.synthesis(ws, noise_mode='const')\n",
        "\n",
        "        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
        "        synth_images = (synth_images + 1) * (255/2)\n",
        "        if synth_images.shape[2] > 256:\n",
        "            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
        "\n",
        "        # Features for synth images.\n",
        "        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n",
        "        dist = (target_features - synth_features).square().sum()\n",
        "\n",
        "        # Noise regularization.\n",
        "        reg_loss = 0.0\n",
        "        for v in noise_bufs.values():\n",
        "            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n",
        "            while True:\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n",
        "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n",
        "                if noise.shape[2] <= 8:\n",
        "                    break\n",
        "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
        "        loss = dist + reg_loss * regularize_noise_weight\n",
        "\n",
        "        # Step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        logprint(f'step {step+1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
        "\n",
        "        # Save projected W for each optimization step.\n",
        "        w_out[step] = w_opt.detach()[0]\n",
        "\n",
        "        # Normalize noise.\n",
        "        with torch.no_grad():\n",
        "            for buf in noise_bufs.values():\n",
        "                buf -= buf.mean()\n",
        "                buf *= buf.square().mean().rsqrt()\n",
        "\n",
        "    return w_out.repeat([1, G.mapping.num_ws, 1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d7ycl_aVyfoZ",
        "outputId": "d2bd4047-a311-4ce3-f6f3-6692ea10e1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/stylegan2-ada-pytorch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir training-runs3"
      ],
      "metadata": {
        "id": "3IzmN8FPeS_N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "adac70d6-92d9-4865-f20a-8ee0890205c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-df758da01dff>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir training-runs3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    165\u001b[0m   \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outdir = '/content/drive/MyDrive/stylegan2-ada-pytorch/training-runs3'"
      ],
      "metadata": {
        "id": "PuI_CwXAyOiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVJZUKBgUOtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0a70245-0b17-4444-b813-504ecdb11295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks\n",
            "Computing W midpoint and stddev using 1000 samples...\n",
            "RecursiveScriptModule(\n",
            "  original_name=VGG16\n",
            "  (layers): RecursiveScriptModule(\n",
            "    original_name=ModuleDict\n",
            "    (conv1): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (conv2): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (pool1): RecursiveScriptModule(original_name=MaxPool2d)\n",
            "    (conv3): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (conv4): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (pool2): RecursiveScriptModule(original_name=MaxPool2d)\n",
            "    (conv5): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (conv6): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (conv7): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (pool3): RecursiveScriptModule(original_name=MaxPool2d)\n",
            "    (conv8): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (conv9): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (conv10): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (pool4): RecursiveScriptModule(original_name=MaxPool2d)\n",
            "    (conv11): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (conv12): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (conv13): RecursiveScriptModule(original_name=Conv2dLayer)\n",
            "    (pool5): RecursiveScriptModule(original_name=MaxPool2d)\n",
            "    (fc1): RecursiveScriptModule(original_name=FullyConnectedLayer)\n",
            "    (fc2): RecursiveScriptModule(original_name=FullyConnectedLayer)\n",
            "    (fc3): RecursiveScriptModule(original_name=Linear)\n",
            "    (softmax): RecursiveScriptModule(original_name=Softmax)\n",
            "  )\n",
            ")\n",
            "step    1/1000: dist 0.71 loss 9557.93\n",
            "step    2/1000: dist 0.67 loss 13249.61\n",
            "step    3/1000: dist 0.68 loss 12875.53\n",
            "step    4/1000: dist 0.66 loss 12156.96\n",
            "step    5/1000: dist 0.71 loss 11157.58\n",
            "step    6/1000: dist 0.67 loss 9953.51\n",
            "step    7/1000: dist 0.69 loss 8621.73\n",
            "step    8/1000: dist 0.72 loss 7229.03\n",
            "step    9/1000: dist 0.65 loss 5841.55\n",
            "step   10/1000: dist 0.65 loss 4524.56\n",
            "step   11/1000: dist 0.64 loss 3363.04\n",
            "step   12/1000: dist 0.67 loss 2427.60\n",
            "step   13/1000: dist 0.63 loss 1747.32\n",
            "step   14/1000: dist 0.66 loss 1339.56\n",
            "step   15/1000: dist 0.67 loss 1195.43\n",
            "step   16/1000: dist 0.66 loss 1275.39\n",
            "step   17/1000: dist 0.64 loss 1512.46\n",
            "step   18/1000: dist 0.62 loss 1807.92\n",
            "step   19/1000: dist 0.65 loss 2065.54\n",
            "step   20/1000: dist 0.59 loss 2253.94\n",
            "step   21/1000: dist 0.59 loss 2354.15\n",
            "step   22/1000: dist 0.61 loss 2367.20\n",
            "step   23/1000: dist 0.63 loss 2298.55\n",
            "step   24/1000: dist 0.70 loss 2140.27\n",
            "step   25/1000: dist 0.58 loss 1914.38\n",
            "step   26/1000: dist 0.70 loss 1660.64\n",
            "step   27/1000: dist 0.57 loss 1425.64\n",
            "step   28/1000: dist 0.57 loss 1211.70\n",
            "step   29/1000: dist 0.59 loss 1010.79\n",
            "step   30/1000: dist 0.58 loss 836.50\n",
            "step   31/1000: dist 0.56 loss 719.59\n",
            "step   32/1000: dist 0.56 loss 652.31\n",
            "step   33/1000: dist 0.56 loss 619.76\n",
            "step   34/1000: dist 0.56 loss 577.54\n",
            "step   35/1000: dist 0.57 loss 531.28\n",
            "step   36/1000: dist 0.56 loss 499.56\n",
            "step   37/1000: dist 0.55 loss 485.19\n",
            "step   38/1000: dist 0.55 loss 495.69\n",
            "step   39/1000: dist 0.56 loss 516.94\n",
            "step   40/1000: dist 0.57 loss 522.98\n",
            "step   41/1000: dist 0.55 loss 514.29\n",
            "step   42/1000: dist 0.56 loss 464.83\n",
            "step   43/1000: dist 0.56 loss 382.52\n",
            "step   44/1000: dist 0.54 loss 301.05\n",
            "step   45/1000: dist 0.54 loss 213.54\n",
            "step   46/1000: dist 0.53 loss 156.26\n",
            "step   47/1000: dist 0.54 loss 138.01\n",
            "step   48/1000: dist 0.53 loss 154.75\n",
            "step   49/1000: dist 0.56 loss 195.98\n",
            "step   50/1000: dist 0.52 loss 215.62\n",
            "step   51/1000: dist 0.56 loss 221.65\n",
            "step   52/1000: dist 0.55 loss 214.53\n",
            "step   53/1000: dist 0.54 loss 164.02\n",
            "step   54/1000: dist 0.55 loss 113.02\n",
            "step   55/1000: dist 0.56 loss 103.59\n",
            "step   56/1000: dist 0.55 loss 96.26\n",
            "step   57/1000: dist 0.57 loss 107.55\n",
            "step   58/1000: dist 0.61 loss 118.72\n",
            "step   59/1000: dist 0.56 loss 89.31\n",
            "step   60/1000: dist 0.54 loss 73.94\n",
            "step   61/1000: dist 0.53 loss 51.01\n",
            "step   62/1000: dist 0.53 loss 48.13\n",
            "step   63/1000: dist 0.55 loss 60.37\n",
            "step   64/1000: dist 0.56 loss 60.05\n",
            "step   65/1000: dist 0.54 loss 64.66\n",
            "step   66/1000: dist 0.54 loss 52.62\n",
            "step   67/1000: dist 0.52 loss 43.56\n",
            "step   68/1000: dist 0.52 loss 31.89\n",
            "step   69/1000: dist 0.54 loss 27.90\n",
            "step   70/1000: dist 0.55 loss 23.06\n",
            "step   71/1000: dist 0.52 loss 18.82\n",
            "step   72/1000: dist 0.52 loss 21.12\n",
            "step   73/1000: dist 0.52 loss 22.86\n",
            "step   74/1000: dist 0.52 loss 25.92\n",
            "step   75/1000: dist 0.52 loss 20.53\n",
            "step   76/1000: dist 0.54 loss 18.67\n",
            "step   77/1000: dist 0.54 loss 14.38\n",
            "step   78/1000: dist 0.52 loss 11.43\n",
            "step   79/1000: dist 0.51 loss 11.91\n",
            "step   80/1000: dist 0.52 loss 10.98\n",
            "step   81/1000: dist 0.53 loss 9.75 \n",
            "step   82/1000: dist 0.51 loss 7.90 \n",
            "step   83/1000: dist 0.54 loss 7.96 \n",
            "step   84/1000: dist 0.53 loss 6.91 \n",
            "step   85/1000: dist 0.54 loss 8.11 \n",
            "step   86/1000: dist 0.54 loss 8.69 \n",
            "step   87/1000: dist 0.52 loss 7.88 \n",
            "step   88/1000: dist 0.53 loss 8.19 \n",
            "step   89/1000: dist 0.51 loss 7.23 \n",
            "step   90/1000: dist 0.51 loss 7.10 \n",
            "step   91/1000: dist 0.54 loss 7.19 \n",
            "step   92/1000: dist 0.53 loss 7.32 \n",
            "step   93/1000: dist 0.53 loss 6.39 \n",
            "step   94/1000: dist 0.51 loss 5.35 \n",
            "step   95/1000: dist 0.51 loss 6.28 \n",
            "step   96/1000: dist 0.52 loss 7.34 \n",
            "step   97/1000: dist 0.51 loss 7.86 \n",
            "step   98/1000: dist 0.51 loss 7.27 \n",
            "step   99/1000: dist 0.52 loss 6.03 \n",
            "step  100/1000: dist 0.53 loss 6.08 \n",
            "step  101/1000: dist 0.51 loss 8.13 \n",
            "step  102/1000: dist 0.50 loss 11.59\n",
            "step  103/1000: dist 0.50 loss 12.68\n",
            "step  104/1000: dist 0.49 loss 8.42 \n",
            "step  105/1000: dist 0.51 loss 4.03 \n",
            "step  106/1000: dist 0.49 loss 5.21 \n",
            "step  107/1000: dist 0.49 loss 7.70 \n",
            "step  108/1000: dist 0.50 loss 5.09 \n",
            "step  109/1000: dist 0.51 loss 1.59 \n",
            "step  110/1000: dist 0.50 loss 3.56 \n",
            "step  111/1000: dist 0.50 loss 5.96 \n",
            "step  112/1000: dist 0.51 loss 3.61 \n",
            "step  113/1000: dist 0.49 loss 1.58 \n",
            "step  114/1000: dist 0.51 loss 3.09 \n",
            "step  115/1000: dist 0.50 loss 3.51 \n",
            "step  116/1000: dist 0.49 loss 2.01 \n",
            "step  117/1000: dist 0.48 loss 2.33 \n",
            "step  118/1000: dist 0.51 loss 3.18 \n",
            "step  119/1000: dist 0.50 loss 1.76 \n",
            "step  120/1000: dist 0.48 loss 0.74 \n",
            "step  121/1000: dist 0.48 loss 1.76 \n",
            "step  122/1000: dist 0.49 loss 2.09 \n",
            "step  123/1000: dist 0.48 loss 1.28 \n",
            "step  124/1000: dist 0.49 loss 1.45 \n",
            "step  125/1000: dist 0.51 loss 1.91 \n",
            "step  126/1000: dist 0.49 loss 1.26 \n",
            "step  127/1000: dist 0.50 loss 0.75 \n",
            "step  128/1000: dist 0.52 loss 1.16 \n",
            "step  129/1000: dist 0.51 loss 1.29 \n",
            "step  130/1000: dist 0.48 loss 0.92 \n",
            "step  131/1000: dist 0.49 loss 1.10 \n",
            "step  132/1000: dist 0.48 loss 1.74 \n",
            "step  133/1000: dist 0.49 loss 2.31 \n",
            "step  134/1000: dist 0.50 loss 3.24 \n",
            "step  135/1000: dist 0.49 loss 4.57 \n",
            "step  136/1000: dist 0.49 loss 4.97 \n",
            "step  137/1000: dist 0.47 loss 4.85 \n",
            "step  138/1000: dist 0.49 loss 7.88 \n",
            "step  139/1000: dist 0.47 loss 15.25\n",
            "step  140/1000: dist 0.48 loss 20.96\n",
            "step  141/1000: dist 0.50 loss 17.66\n",
            "step  142/1000: dist 0.47 loss 13.58\n",
            "step  143/1000: dist 0.47 loss 19.97\n",
            "step  144/1000: dist 0.47 loss 22.62\n",
            "step  145/1000: dist 0.47 loss 10.63\n",
            "step  146/1000: dist 0.50 loss 5.09 \n",
            "step  147/1000: dist 0.47 loss 14.42\n",
            "step  148/1000: dist 0.48 loss 14.80\n",
            "step  149/1000: dist 0.46 loss 3.33 \n",
            "step  150/1000: dist 0.47 loss 3.68 \n",
            "step  151/1000: dist 0.46 loss 10.07\n",
            "step  152/1000: dist 0.47 loss 5.40 \n",
            "step  153/1000: dist 0.47 loss 2.37 \n",
            "step  154/1000: dist 0.47 loss 7.67 \n",
            "step  155/1000: dist 0.45 loss 6.41 \n",
            "step  156/1000: dist 0.46 loss 1.81 \n",
            "step  157/1000: dist 0.45 loss 4.32 \n",
            "step  158/1000: dist 0.47 loss 5.25 \n",
            "step  159/1000: dist 0.47 loss 2.26 \n",
            "step  160/1000: dist 0.46 loss 4.62 \n",
            "step  161/1000: dist 0.46 loss 8.59 \n",
            "step  162/1000: dist 0.47 loss 9.39 \n",
            "step  163/1000: dist 0.45 loss 13.24\n",
            "step  164/1000: dist 0.46 loss 19.77\n",
            "step  165/1000: dist 0.46 loss 22.71\n",
            "step  166/1000: dist 0.46 loss 23.36\n",
            "step  167/1000: dist 0.48 loss 22.53\n",
            "step  168/1000: dist 0.50 loss 17.46\n",
            "step  169/1000: dist 0.45 loss 11.42\n",
            "step  170/1000: dist 0.48 loss 11.53\n",
            "step  171/1000: dist 0.46 loss 15.98\n",
            "step  172/1000: dist 0.47 loss 18.28\n",
            "step  173/1000: dist 0.45 loss 17.45\n",
            "step  174/1000: dist 0.45 loss 14.96\n",
            "step  175/1000: dist 0.46 loss 10.48\n",
            "step  176/1000: dist 0.45 loss 6.49 \n",
            "step  177/1000: dist 0.45 loss 7.45 \n",
            "step  178/1000: dist 0.48 loss 11.04\n",
            "step  179/1000: dist 0.44 loss 8.73 \n",
            "step  180/1000: dist 0.45 loss 2.96 \n",
            "step  181/1000: dist 0.44 loss 4.16 \n",
            "step  182/1000: dist 0.44 loss 8.64 \n",
            "step  183/1000: dist 0.44 loss 5.76 \n",
            "step  184/1000: dist 0.46 loss 1.25 \n",
            "step  185/1000: dist 0.49 loss 3.84 \n",
            "step  186/1000: dist 0.44 loss 6.19 \n",
            "step  187/1000: dist 0.45 loss 3.05 \n",
            "step  188/1000: dist 0.43 loss 2.21 \n",
            "step  189/1000: dist 0.44 loss 5.91 \n",
            "step  190/1000: dist 0.46 loss 7.96 \n",
            "step  191/1000: dist 0.48 loss 9.52 \n",
            "step  192/1000: dist 0.45 loss 13.40\n",
            "step  193/1000: dist 0.44 loss 15.00\n",
            "step  194/1000: dist 0.45 loss 15.73\n",
            "step  195/1000: dist 0.45 loss 20.27\n",
            "step  196/1000: dist 0.45 loss 18.87\n",
            "step  197/1000: dist 0.44 loss 8.55 \n",
            "step  198/1000: dist 0.46 loss 6.57 \n",
            "step  199/1000: dist 0.46 loss 17.40\n",
            "step  200/1000: dist 0.45 loss 22.87\n",
            "step  201/1000: dist 0.45 loss 16.95\n",
            "step  202/1000: dist 0.44 loss 12.23\n",
            "step  203/1000: dist 0.43 loss 9.92 \n",
            "step  204/1000: dist 0.44 loss 6.18 \n",
            "step  205/1000: dist 0.44 loss 7.34 \n",
            "step  206/1000: dist 0.43 loss 11.23\n",
            "step  207/1000: dist 0.43 loss 6.97 \n",
            "step  208/1000: dist 0.43 loss 2.18 \n",
            "step  209/1000: dist 0.45 loss 6.09 \n",
            "step  210/1000: dist 0.43 loss 7.72 \n",
            "step  211/1000: dist 0.43 loss 2.93 \n",
            "step  212/1000: dist 0.43 loss 2.52 \n",
            "step  213/1000: dist 0.43 loss 5.34 \n",
            "step  214/1000: dist 0.44 loss 3.75 \n",
            "step  215/1000: dist 0.43 loss 1.85 \n",
            "step  216/1000: dist 0.43 loss 3.12 \n",
            "step  217/1000: dist 0.44 loss 3.32 \n",
            "step  218/1000: dist 0.44 loss 2.10 \n",
            "step  219/1000: dist 0.42 loss 2.03 \n",
            "step  220/1000: dist 0.43 loss 2.35 \n",
            "step  221/1000: dist 0.47 loss 2.23 \n",
            "step  222/1000: dist 0.43 loss 2.07 \n",
            "step  223/1000: dist 0.45 loss 2.09 \n",
            "step  224/1000: dist 0.43 loss 2.47 \n",
            "step  225/1000: dist 0.42 loss 3.17 \n",
            "step  226/1000: dist 0.44 loss 3.91 \n",
            "step  227/1000: dist 0.43 loss 5.47 \n",
            "step  228/1000: dist 0.44 loss 8.01 \n",
            "step  229/1000: dist 0.42 loss 9.89 \n",
            "step  230/1000: dist 0.42 loss 10.26\n",
            "step  231/1000: dist 0.43 loss 9.01 \n",
            "step  232/1000: dist 0.43 loss 6.38 \n",
            "step  233/1000: dist 0.43 loss 3.83 \n",
            "step  234/1000: dist 0.43 loss 3.10 \n",
            "step  235/1000: dist 0.44 loss 3.57 \n",
            "step  236/1000: dist 0.42 loss 3.58 \n",
            "step  237/1000: dist 0.41 loss 3.52 \n",
            "step  238/1000: dist 0.42 loss 4.18 \n",
            "step  239/1000: dist 0.42 loss 5.01 \n",
            "step  240/1000: dist 0.41 loss 6.19 \n",
            "step  241/1000: dist 0.42 loss 9.31 \n",
            "step  242/1000: dist 0.43 loss 14.70\n",
            "step  243/1000: dist 0.42 loss 19.01\n",
            "step  244/1000: dist 0.42 loss 15.87\n",
            "step  245/1000: dist 0.41 loss 7.97 \n",
            "step  246/1000: dist 0.42 loss 8.18 \n",
            "step  247/1000: dist 0.42 loss 20.14\n",
            "step  248/1000: dist 0.41 loss 30.15\n",
            "step  249/1000: dist 0.41 loss 23.72\n",
            "step  250/1000: dist 0.41 loss 7.73 \n",
            "step  251/1000: dist 0.41 loss 5.35 \n",
            "step  252/1000: dist 0.41 loss 13.90\n",
            "step  253/1000: dist 0.41 loss 12.50\n",
            "step  254/1000: dist 0.44 loss 4.22 \n",
            "step  255/1000: dist 0.42 loss 6.24 \n",
            "step  256/1000: dist 0.41 loss 10.37\n",
            "step  257/1000: dist 0.40 loss 4.58 \n",
            "step  258/1000: dist 0.41 loss 2.35 \n",
            "step  259/1000: dist 0.41 loss 7.33 \n",
            "step  260/1000: dist 0.41 loss 5.09 \n",
            "step  261/1000: dist 0.40 loss 1.44 \n",
            "step  262/1000: dist 0.42 loss 4.91 \n",
            "step  263/1000: dist 0.40 loss 4.19 \n",
            "step  264/1000: dist 0.40 loss 0.75 \n",
            "step  265/1000: dist 0.40 loss 3.49 \n",
            "step  266/1000: dist 0.41 loss 3.83 \n",
            "step  267/1000: dist 0.41 loss 0.72 \n",
            "step  268/1000: dist 0.40 loss 2.28 \n",
            "step  269/1000: dist 0.41 loss 2.93 \n",
            "step  270/1000: dist 0.40 loss 0.63 \n",
            "step  271/1000: dist 0.41 loss 1.69 \n",
            "step  272/1000: dist 0.41 loss 2.48 \n",
            "step  273/1000: dist 0.40 loss 0.82 \n",
            "step  274/1000: dist 0.40 loss 1.27 \n",
            "step  275/1000: dist 0.41 loss 1.92 \n",
            "step  276/1000: dist 0.40 loss 1.08 \n",
            "step  277/1000: dist 0.44 loss 1.65 \n",
            "step  278/1000: dist 0.41 loss 2.55 \n",
            "step  279/1000: dist 0.40 loss 3.10 \n",
            "step  280/1000: dist 0.40 loss 5.39 \n",
            "step  281/1000: dist 0.41 loss 9.34 \n",
            "step  282/1000: dist 0.40 loss 15.21\n",
            "step  283/1000: dist 0.41 loss 22.76\n",
            "step  284/1000: dist 0.40 loss 26.95\n",
            "step  285/1000: dist 0.41 loss 22.19\n",
            "step  286/1000: dist 0.40 loss 11.42\n",
            "step  287/1000: dist 0.40 loss 4.16 \n",
            "step  288/1000: dist 0.40 loss 6.27 \n",
            "step  289/1000: dist 0.40 loss 11.38\n",
            "step  290/1000: dist 0.40 loss 9.88 \n",
            "step  291/1000: dist 0.40 loss 4.46 \n",
            "step  292/1000: dist 0.39 loss 3.55 \n",
            "step  293/1000: dist 0.40 loss 6.27 \n",
            "step  294/1000: dist 0.40 loss 6.11 \n",
            "step  295/1000: dist 0.41 loss 3.36 \n",
            "step  296/1000: dist 0.39 loss 2.68 \n",
            "step  297/1000: dist 0.40 loss 4.02 \n",
            "step  298/1000: dist 0.40 loss 3.81 \n",
            "step  299/1000: dist 0.40 loss 2.29 \n",
            "step  300/1000: dist 0.41 loss 2.12 \n",
            "step  301/1000: dist 0.40 loss 2.84 \n",
            "step  302/1000: dist 0.40 loss 2.42 \n",
            "step  303/1000: dist 0.40 loss 1.57 \n",
            "step  304/1000: dist 0.39 loss 1.78 \n",
            "step  305/1000: dist 0.40 loss 2.16 \n",
            "step  306/1000: dist 0.39 loss 1.78 \n",
            "step  307/1000: dist 0.40 loss 1.65 \n",
            "step  308/1000: dist 0.40 loss 2.31 \n",
            "step  309/1000: dist 0.40 loss 3.11 \n",
            "step  310/1000: dist 0.40 loss 4.23 \n",
            "step  311/1000: dist 0.39 loss 6.72 \n",
            "step  312/1000: dist 0.39 loss 10.49\n",
            "step  313/1000: dist 0.40 loss 13.83\n",
            "step  314/1000: dist 0.39 loss 13.64\n",
            "step  315/1000: dist 0.39 loss 8.88 \n",
            "step  316/1000: dist 0.38 loss 4.39 \n",
            "step  317/1000: dist 0.39 loss 6.45 \n",
            "step  318/1000: dist 0.41 loss 13.95\n",
            "step  319/1000: dist 0.39 loss 19.53\n",
            "step  320/1000: dist 0.39 loss 18.07\n",
            "step  321/1000: dist 0.39 loss 11.18\n",
            "step  322/1000: dist 0.39 loss 5.75 \n",
            "step  323/1000: dist 0.41 loss 5.95 \n",
            "step  324/1000: dist 0.40 loss 9.30 \n",
            "step  325/1000: dist 0.38 loss 12.19\n",
            "step  326/1000: dist 0.39 loss 14.07\n",
            "step  327/1000: dist 0.39 loss 17.08\n",
            "step  328/1000: dist 0.39 loss 21.83\n",
            "step  329/1000: dist 0.43 loss 23.75\n",
            "step  330/1000: dist 0.39 loss 17.84\n",
            "step  331/1000: dist 0.38 loss 9.96 \n",
            "step  332/1000: dist 0.40 loss 8.67 \n",
            "step  333/1000: dist 0.39 loss 11.56\n",
            "step  334/1000: dist 0.38 loss 10.45\n",
            "step  335/1000: dist 0.38 loss 6.23 \n",
            "step  336/1000: dist 0.40 loss 8.02 \n",
            "step  337/1000: dist 0.38 loss 15.32\n",
            "step  338/1000: dist 0.39 loss 19.19\n",
            "step  339/1000: dist 0.40 loss 18.83\n",
            "step  340/1000: dist 0.38 loss 20.17\n",
            "step  341/1000: dist 0.39 loss 22.81\n",
            "step  342/1000: dist 0.39 loss 25.29\n",
            "step  343/1000: dist 0.39 loss 29.57\n",
            "step  344/1000: dist 0.38 loss 31.45\n",
            "step  345/1000: dist 0.38 loss 23.97\n",
            "step  346/1000: dist 0.39 loss 13.73\n",
            "step  347/1000: dist 0.39 loss 12.12\n",
            "step  348/1000: dist 0.39 loss 15.01\n",
            "step  349/1000: dist 0.38 loss 11.69\n",
            "step  350/1000: dist 0.40 loss 7.99 \n",
            "step  351/1000: dist 0.40 loss 12.73\n",
            "step  352/1000: dist 0.41 loss 17.31\n",
            "step  353/1000: dist 0.39 loss 15.63\n",
            "step  354/1000: dist 0.38 loss 18.02\n",
            "step  355/1000: dist 0.40 loss 24.56\n",
            "step  356/1000: dist 0.44 loss 19.95\n",
            "step  357/1000: dist 0.38 loss 6.51 \n",
            "step  358/1000: dist 0.37 loss 3.30 \n",
            "step  359/1000: dist 0.38 loss 10.33\n",
            "step  360/1000: dist 0.38 loss 12.50\n",
            "step  361/1000: dist 0.38 loss 6.61 \n",
            "step  362/1000: dist 0.38 loss 2.73 \n",
            "step  363/1000: dist 0.38 loss 5.53 \n",
            "step  364/1000: dist 0.38 loss 7.87 \n",
            "step  365/1000: dist 0.38 loss 4.50 \n",
            "step  366/1000: dist 0.37 loss 1.65 \n",
            "step  367/1000: dist 0.37 loss 4.11 \n",
            "step  368/1000: dist 0.37 loss 5.36 \n",
            "step  369/1000: dist 0.38 loss 2.34 \n",
            "step  370/1000: dist 0.37 loss 1.60 \n",
            "step  371/1000: dist 0.38 loss 3.65 \n",
            "step  372/1000: dist 0.38 loss 3.07 \n",
            "step  373/1000: dist 0.37 loss 1.81 \n",
            "step  374/1000: dist 0.38 loss 3.16 \n",
            "step  375/1000: dist 0.38 loss 4.15 \n",
            "step  376/1000: dist 0.38 loss 4.55 \n",
            "step  377/1000: dist 0.38 loss 7.77 \n",
            "step  378/1000: dist 0.39 loss 12.71\n",
            "step  379/1000: dist 0.40 loss 17.08\n",
            "step  380/1000: dist 0.38 loss 20.81\n",
            "step  381/1000: dist 0.37 loss 21.55\n",
            "step  382/1000: dist 0.38 loss 19.55\n",
            "step  383/1000: dist 0.39 loss 20.21\n",
            "step  384/1000: dist 0.37 loss 20.70\n",
            "step  385/1000: dist 0.37 loss 13.91\n",
            "step  386/1000: dist 0.38 loss 5.36 \n",
            "step  387/1000: dist 0.40 loss 5.67 \n",
            "step  388/1000: dist 0.38 loss 11.31\n",
            "step  389/1000: dist 0.37 loss 10.53\n",
            "step  390/1000: dist 0.38 loss 4.50 \n",
            "step  391/1000: dist 0.38 loss 3.58 \n",
            "step  392/1000: dist 0.37 loss 7.09 \n",
            "step  393/1000: dist 0.38 loss 6.77 \n",
            "step  394/1000: dist 0.37 loss 3.85 \n",
            "step  395/1000: dist 0.37 loss 4.76 \n",
            "step  396/1000: dist 0.37 loss 7.58 \n",
            "step  397/1000: dist 0.37 loss 8.41 \n",
            "step  398/1000: dist 0.37 loss 10.10\n",
            "step  399/1000: dist 0.37 loss 13.72\n",
            "step  400/1000: dist 0.38 loss 14.63\n",
            "step  401/1000: dist 0.37 loss 10.60\n",
            "step  402/1000: dist 0.38 loss 5.31 \n",
            "step  403/1000: dist 0.39 loss 2.21 \n",
            "step  404/1000: dist 0.37 loss 2.91 \n",
            "step  405/1000: dist 0.37 loss 6.18 \n",
            "step  406/1000: dist 0.37 loss 6.97 \n",
            "step  407/1000: dist 0.36 loss 3.49 \n",
            "step  408/1000: dist 0.37 loss 1.27 \n",
            "step  409/1000: dist 0.37 loss 3.47 \n",
            "step  410/1000: dist 0.37 loss 6.11 \n",
            "step  411/1000: dist 0.37 loss 6.75 \n",
            "step  412/1000: dist 0.37 loss 7.98 \n",
            "step  413/1000: dist 0.37 loss 10.19\n",
            "step  414/1000: dist 0.37 loss 9.33 \n",
            "step  415/1000: dist 0.37 loss 4.76 \n",
            "step  416/1000: dist 0.37 loss 1.61 \n",
            "step  417/1000: dist 0.37 loss 3.04 \n",
            "step  418/1000: dist 0.37 loss 5.77 \n",
            "step  419/1000: dist 0.37 loss 4.97 \n",
            "step  420/1000: dist 0.36 loss 1.69 \n",
            "step  421/1000: dist 0.37 loss 1.19 \n",
            "step  422/1000: dist 0.36 loss 3.47 \n",
            "step  423/1000: dist 0.36 loss 3.69 \n",
            "step  424/1000: dist 0.36 loss 1.35 \n",
            "step  425/1000: dist 0.39 loss 0.82 \n",
            "step  426/1000: dist 0.37 loss 2.40 \n",
            "step  427/1000: dist 0.37 loss 2.51 \n",
            "step  428/1000: dist 0.37 loss 0.98 \n",
            "step  429/1000: dist 0.38 loss 0.76 \n",
            "step  430/1000: dist 0.39 loss 1.76 \n",
            "step  431/1000: dist 0.36 loss 1.64 \n",
            "step  432/1000: dist 0.36 loss 0.73 \n",
            "step  433/1000: dist 0.36 loss 0.74 \n",
            "step  434/1000: dist 0.36 loss 1.28 \n",
            "step  435/1000: dist 0.36 loss 1.09 \n",
            "step  436/1000: dist 0.37 loss 0.63 \n",
            "step  437/1000: dist 0.37 loss 0.72 \n",
            "step  438/1000: dist 0.36 loss 0.95 \n",
            "step  439/1000: dist 0.38 loss 0.78 \n",
            "step  440/1000: dist 0.38 loss 0.56 \n",
            "step  441/1000: dist 0.37 loss 0.66 \n",
            "step  442/1000: dist 0.37 loss 0.75 \n",
            "step  443/1000: dist 0.38 loss 0.61 \n",
            "step  444/1000: dist 0.36 loss 0.50 \n",
            "step  445/1000: dist 0.36 loss 0.59 \n",
            "step  446/1000: dist 0.37 loss 0.64 \n",
            "step  447/1000: dist 0.37 loss 0.52 \n",
            "step  448/1000: dist 0.35 loss 0.48 \n",
            "step  449/1000: dist 0.36 loss 0.61 \n",
            "step  450/1000: dist 0.36 loss 0.70 \n",
            "step  451/1000: dist 0.36 loss 0.73 \n",
            "step  452/1000: dist 0.36 loss 0.95 \n",
            "step  453/1000: dist 0.36 loss 1.52 \n",
            "step  454/1000: dist 0.36 loss 2.47 \n",
            "step  455/1000: dist 0.35 loss 4.08 \n",
            "step  456/1000: dist 0.35 loss 6.89 \n",
            "step  457/1000: dist 0.36 loss 11.02\n",
            "step  458/1000: dist 0.35 loss 15.15\n",
            "step  459/1000: dist 0.36 loss 16.06\n",
            "step  460/1000: dist 0.36 loss 12.38\n",
            "step  461/1000: dist 0.36 loss 8.97 \n",
            "step  462/1000: dist 0.35 loss 12.95\n",
            "step  463/1000: dist 0.35 loss 23.21\n",
            "step  464/1000: dist 0.35 loss 28.01\n",
            "step  465/1000: dist 0.35 loss 18.54\n",
            "step  466/1000: dist 0.36 loss 3.66 \n",
            "step  467/1000: dist 0.36 loss 1.85 \n",
            "step  468/1000: dist 0.35 loss 11.36\n",
            "step  469/1000: dist 0.36 loss 13.87\n",
            "step  470/1000: dist 0.36 loss 4.96 \n",
            "step  471/1000: dist 0.36 loss 0.56 \n",
            "step  472/1000: dist 0.36 loss 6.51 \n",
            "step  473/1000: dist 0.35 loss 8.79 \n",
            "step  474/1000: dist 0.35 loss 2.77 \n",
            "step  475/1000: dist 0.35 loss 0.89 \n",
            "step  476/1000: dist 0.36 loss 5.11 \n",
            "step  477/1000: dist 0.35 loss 4.88 \n",
            "step  478/1000: dist 0.35 loss 1.19 \n",
            "step  479/1000: dist 0.35 loss 1.82 \n",
            "step  480/1000: dist 0.35 loss 3.63 \n",
            "step  481/1000: dist 0.35 loss 2.12 \n",
            "step  482/1000: dist 0.35 loss 1.22 \n",
            "step  483/1000: dist 0.36 loss 2.24 \n",
            "step  484/1000: dist 0.35 loss 1.85 \n",
            "step  485/1000: dist 0.35 loss 1.25 \n",
            "step  486/1000: dist 0.37 loss 2.09 \n",
            "step  487/1000: dist 0.36 loss 2.15 \n",
            "step  488/1000: dist 0.35 loss 2.05 \n",
            "step  489/1000: dist 0.36 loss 4.22 \n",
            "step  490/1000: dist 0.36 loss 7.59 \n",
            "step  491/1000: dist 0.35 loss 12.65\n",
            "step  492/1000: dist 0.35 loss 23.29\n",
            "step  493/1000: dist 0.35 loss 38.05\n",
            "step  494/1000: dist 0.35 loss 47.54\n",
            "step  495/1000: dist 0.34 loss 41.49\n",
            "step  496/1000: dist 0.35 loss 25.66\n",
            "step  497/1000: dist 0.35 loss 20.15\n",
            "step  498/1000: dist 0.35 loss 31.33\n",
            "step  499/1000: dist 0.35 loss 38.92\n",
            "step  500/1000: dist 0.35 loss 26.76\n",
            "step  501/1000: dist 0.35 loss 10.35\n",
            "step  502/1000: dist 0.35 loss 8.94 \n",
            "step  503/1000: dist 0.37 loss 16.02\n",
            "step  504/1000: dist 0.35 loss 17.28\n",
            "step  505/1000: dist 0.34 loss 10.77\n",
            "step  506/1000: dist 0.37 loss 5.30 \n",
            "step  507/1000: dist 0.34 loss 8.12 \n",
            "step  508/1000: dist 0.35 loss 11.95\n",
            "step  509/1000: dist 0.34 loss 6.74 \n",
            "step  510/1000: dist 0.36 loss 2.40 \n",
            "step  511/1000: dist 0.37 loss 7.51 \n",
            "step  512/1000: dist 0.36 loss 8.72 \n",
            "step  513/1000: dist 0.35 loss 3.05 \n",
            "step  514/1000: dist 0.37 loss 5.23 \n",
            "step  515/1000: dist 0.37 loss 11.40\n",
            "step  516/1000: dist 0.34 loss 10.95\n",
            "step  517/1000: dist 0.35 loss 13.72\n",
            "step  518/1000: dist 0.35 loss 21.63\n",
            "step  519/1000: dist 0.34 loss 20.10\n",
            "step  520/1000: dist 0.35 loss 9.56 \n",
            "step  521/1000: dist 0.35 loss 3.90 \n",
            "step  522/1000: dist 0.34 loss 4.78 \n",
            "step  523/1000: dist 0.35 loss 8.30 \n",
            "step  524/1000: dist 0.35 loss 11.40\n",
            "step  525/1000: dist 0.34 loss 9.69 \n",
            "step  526/1000: dist 0.35 loss 9.06 \n",
            "step  527/1000: dist 0.36 loss 18.50\n",
            "step  528/1000: dist 0.34 loss 32.84\n",
            "step  529/1000: dist 0.34 loss 42.48\n",
            "step  530/1000: dist 0.35 loss 44.20\n",
            "step  531/1000: dist 0.37 loss 31.82\n",
            "step  532/1000: dist 0.35 loss 11.10\n",
            "step  533/1000: dist 0.34 loss 5.15 \n",
            "step  534/1000: dist 0.35 loss 19.14\n",
            "step  535/1000: dist 0.36 loss 29.42\n",
            "step  536/1000: dist 0.34 loss 23.35\n",
            "step  537/1000: dist 0.34 loss 21.36\n",
            "step  538/1000: dist 0.35 loss 32.54\n",
            "step  539/1000: dist 0.34 loss 30.78\n",
            "step  540/1000: dist 0.34 loss 10.01\n",
            "step  541/1000: dist 0.34 loss 2.15 \n",
            "step  542/1000: dist 0.34 loss 14.90\n",
            "step  543/1000: dist 0.34 loss 19.20\n",
            "step  544/1000: dist 0.35 loss 7.60 \n",
            "step  545/1000: dist 0.34 loss 3.02 \n",
            "step  546/1000: dist 0.34 loss 9.57 \n",
            "step  547/1000: dist 0.34 loss 10.39\n",
            "step  548/1000: dist 0.34 loss 4.43 \n",
            "step  549/1000: dist 0.34 loss 3.42 \n",
            "step  550/1000: dist 0.35 loss 6.71 \n",
            "step  551/1000: dist 0.34 loss 5.92 \n",
            "step  552/1000: dist 0.33 loss 2.74 \n",
            "step  553/1000: dist 0.35 loss 3.50 \n",
            "step  554/1000: dist 0.35 loss 5.61 \n",
            "step  555/1000: dist 0.34 loss 4.63 \n",
            "step  556/1000: dist 0.34 loss 4.60 \n",
            "step  557/1000: dist 0.34 loss 8.95 \n",
            "step  558/1000: dist 0.33 loss 14.18\n",
            "step  559/1000: dist 0.34 loss 19.22\n",
            "step  560/1000: dist 0.34 loss 25.33\n",
            "step  561/1000: dist 0.34 loss 24.03\n",
            "step  562/1000: dist 0.34 loss 12.36\n",
            "step  563/1000: dist 0.35 loss 5.38 \n",
            "step  564/1000: dist 0.34 loss 13.00\n",
            "step  565/1000: dist 0.34 loss 22.00\n",
            "step  566/1000: dist 0.34 loss 19.28\n",
            "step  567/1000: dist 0.34 loss 12.04\n",
            "step  568/1000: dist 0.33 loss 10.11\n",
            "step  569/1000: dist 0.34 loss 9.26 \n",
            "step  570/1000: dist 0.34 loss 4.58 \n",
            "step  571/1000: dist 0.33 loss 2.81 \n",
            "step  572/1000: dist 0.34 loss 7.52 \n",
            "step  573/1000: dist 0.33 loss 9.40 \n",
            "step  574/1000: dist 0.33 loss 4.56 \n",
            "step  575/1000: dist 0.33 loss 1.69 \n",
            "step  576/1000: dist 0.33 loss 3.52 \n",
            "step  577/1000: dist 0.33 loss 4.12 \n",
            "step  578/1000: dist 0.33 loss 3.43 \n",
            "step  579/1000: dist 0.33 loss 3.66 \n",
            "step  580/1000: dist 0.33 loss 2.48 \n",
            "step  581/1000: dist 0.33 loss 0.93 \n",
            "step  582/1000: dist 0.33 loss 2.22 \n",
            "step  583/1000: dist 0.33 loss 3.40 \n",
            "step  584/1000: dist 0.33 loss 1.93 \n",
            "step  585/1000: dist 0.33 loss 1.08 \n",
            "step  586/1000: dist 0.33 loss 2.04 \n",
            "step  587/1000: dist 0.33 loss 2.67 \n",
            "step  588/1000: dist 0.33 loss 3.53 \n",
            "step  589/1000: dist 0.33 loss 5.65 \n",
            "step  590/1000: dist 0.33 loss 8.81 \n",
            "step  591/1000: dist 0.33 loss 14.42\n",
            "step  592/1000: dist 0.33 loss 22.15\n",
            "step  593/1000: dist 0.33 loss 24.68\n",
            "step  594/1000: dist 0.33 loss 16.70\n",
            "step  595/1000: dist 0.33 loss 4.86 \n",
            "step  596/1000: dist 0.33 loss 3.02 \n",
            "step  597/1000: dist 0.33 loss 11.29\n",
            "step  598/1000: dist 0.33 loss 16.61\n",
            "step  599/1000: dist 0.33 loss 13.57\n",
            "step  600/1000: dist 0.33 loss 13.40\n",
            "step  601/1000: dist 0.33 loss 21.47\n",
            "step  602/1000: dist 0.33 loss 23.37\n",
            "step  603/1000: dist 0.33 loss 11.89\n",
            "step  604/1000: dist 0.33 loss 2.78 \n",
            "step  605/1000: dist 0.33 loss 5.61 \n",
            "step  606/1000: dist 0.33 loss 10.25\n",
            "step  607/1000: dist 0.32 loss 10.98\n",
            "step  608/1000: dist 0.33 loss 11.45\n",
            "step  609/1000: dist 0.32 loss 13.01\n",
            "step  610/1000: dist 0.32 loss 17.71\n",
            "step  611/1000: dist 0.33 loss 25.83\n",
            "step  612/1000: dist 0.33 loss 26.30\n",
            "step  613/1000: dist 0.33 loss 13.16\n",
            "step  614/1000: dist 0.33 loss 3.04 \n",
            "step  615/1000: dist 0.32 loss 8.21 \n",
            "step  616/1000: dist 0.33 loss 16.46\n",
            "step  617/1000: dist 0.32 loss 16.40\n",
            "step  618/1000: dist 0.32 loss 15.50\n",
            "step  619/1000: dist 0.32 loss 23.22\n",
            "step  620/1000: dist 0.33 loss 33.74\n",
            "step  621/1000: dist 0.32 loss 34.10\n",
            "step  622/1000: dist 0.32 loss 22.00\n",
            "step  623/1000: dist 0.32 loss 8.95 \n",
            "step  624/1000: dist 0.32 loss 5.45 \n",
            "step  625/1000: dist 0.32 loss 9.22 \n",
            "step  626/1000: dist 0.32 loss 13.97\n",
            "step  627/1000: dist 0.32 loss 13.61\n",
            "step  628/1000: dist 0.32 loss 5.92 \n",
            "step  629/1000: dist 0.32 loss 1.42 \n",
            "step  630/1000: dist 0.32 loss 7.08 \n",
            "step  631/1000: dist 0.32 loss 10.69\n",
            "step  632/1000: dist 0.32 loss 4.43 \n",
            "step  633/1000: dist 0.32 loss 0.57 \n",
            "step  634/1000: dist 0.32 loss 4.84 \n",
            "step  635/1000: dist 0.32 loss 6.62 \n",
            "step  636/1000: dist 0.32 loss 3.02 \n",
            "step  637/1000: dist 0.32 loss 1.76 \n",
            "step  638/1000: dist 0.32 loss 3.87 \n",
            "step  639/1000: dist 0.33 loss 5.02 \n",
            "step  640/1000: dist 0.32 loss 5.14 \n",
            "step  641/1000: dist 0.33 loss 6.63 \n",
            "step  642/1000: dist 0.32 loss 10.66\n",
            "step  643/1000: dist 0.33 loss 16.65\n",
            "step  644/1000: dist 0.32 loss 21.30\n",
            "step  645/1000: dist 0.33 loss 20.57\n",
            "step  646/1000: dist 0.32 loss 13.06\n",
            "step  647/1000: dist 0.32 loss 4.05 \n",
            "step  648/1000: dist 0.32 loss 1.79 \n",
            "step  649/1000: dist 0.32 loss 6.93 \n",
            "step  650/1000: dist 0.32 loss 10.26\n",
            "step  651/1000: dist 0.32 loss 5.74 \n",
            "step  652/1000: dist 0.32 loss 0.89 \n",
            "step  653/1000: dist 0.32 loss 3.12 \n",
            "step  654/1000: dist 0.33 loss 6.31 \n",
            "step  655/1000: dist 0.32 loss 3.73 \n",
            "step  656/1000: dist 0.33 loss 0.73 \n",
            "step  657/1000: dist 0.32 loss 2.56 \n",
            "step  658/1000: dist 0.32 loss 4.17 \n",
            "step  659/1000: dist 0.32 loss 1.98 \n",
            "step  660/1000: dist 0.32 loss 0.94 \n",
            "step  661/1000: dist 0.32 loss 2.95 \n",
            "step  662/1000: dist 0.32 loss 3.57 \n",
            "step  663/1000: dist 0.32 loss 2.69 \n",
            "step  664/1000: dist 0.32 loss 4.56 \n",
            "step  665/1000: dist 0.32 loss 8.72 \n",
            "step  666/1000: dist 0.32 loss 12.58\n",
            "step  667/1000: dist 0.32 loss 16.99\n",
            "step  668/1000: dist 0.32 loss 20.78\n",
            "step  669/1000: dist 0.32 loss 17.24\n",
            "step  670/1000: dist 0.32 loss 6.95 \n",
            "step  671/1000: dist 0.32 loss 0.67 \n",
            "step  672/1000: dist 0.32 loss 3.78 \n",
            "step  673/1000: dist 0.32 loss 9.17 \n",
            "step  674/1000: dist 0.32 loss 8.69 \n",
            "step  675/1000: dist 0.32 loss 3.41 \n",
            "step  676/1000: dist 0.32 loss 0.86 \n",
            "step  677/1000: dist 0.31 loss 3.63 \n",
            "step  678/1000: dist 0.32 loss 5.97 \n",
            "step  679/1000: dist 0.32 loss 3.53 \n",
            "step  680/1000: dist 0.31 loss 0.58 \n",
            "step  681/1000: dist 0.31 loss 1.82 \n",
            "step  682/1000: dist 0.31 loss 3.96 \n",
            "step  683/1000: dist 0.31 loss 2.60 \n",
            "step  684/1000: dist 0.31 loss 0.45 \n",
            "step  685/1000: dist 0.31 loss 1.29 \n",
            "step  686/1000: dist 0.31 loss 2.71 \n",
            "step  687/1000: dist 0.31 loss 1.69 \n",
            "step  688/1000: dist 0.31 loss 0.38 \n",
            "step  689/1000: dist 0.31 loss 1.08 \n",
            "step  690/1000: dist 0.31 loss 1.88 \n",
            "step  691/1000: dist 0.31 loss 1.09 \n",
            "step  692/1000: dist 0.31 loss 0.37 \n",
            "step  693/1000: dist 0.31 loss 0.92 \n",
            "step  694/1000: dist 0.31 loss 1.32 \n",
            "step  695/1000: dist 0.31 loss 0.75 \n",
            "step  696/1000: dist 0.31 loss 0.38 \n",
            "step  697/1000: dist 0.31 loss 0.77 \n",
            "step  698/1000: dist 0.31 loss 0.96 \n",
            "step  699/1000: dist 0.31 loss 0.56 \n",
            "step  700/1000: dist 0.31 loss 0.37 \n",
            "step  701/1000: dist 0.31 loss 0.64 \n",
            "step  702/1000: dist 0.31 loss 0.73 \n",
            "step  703/1000: dist 0.31 loss 0.46 \n",
            "step  704/1000: dist 0.31 loss 0.36 \n",
            "step  705/1000: dist 0.31 loss 0.54 \n",
            "step  706/1000: dist 0.31 loss 0.58 \n",
            "step  707/1000: dist 0.31 loss 0.40 \n",
            "step  708/1000: dist 0.32 loss 0.35 \n",
            "step  709/1000: dist 0.31 loss 0.48 \n",
            "step  710/1000: dist 0.31 loss 0.49 \n",
            "step  711/1000: dist 0.31 loss 0.36 \n",
            "step  712/1000: dist 0.31 loss 0.33 \n",
            "step  713/1000: dist 0.31 loss 0.43 \n",
            "step  714/1000: dist 0.31 loss 0.43 \n",
            "step  715/1000: dist 0.31 loss 0.34 \n",
            "step  716/1000: dist 0.31 loss 0.32 \n",
            "step  717/1000: dist 0.32 loss 0.39 \n",
            "step  718/1000: dist 0.31 loss 0.40 \n",
            "step  719/1000: dist 0.31 loss 0.33 \n",
            "step  720/1000: dist 0.31 loss 0.32 \n",
            "step  721/1000: dist 0.31 loss 0.36 \n",
            "step  722/1000: dist 0.31 loss 0.37 \n",
            "step  723/1000: dist 0.31 loss 0.33 \n",
            "step  724/1000: dist 0.31 loss 0.32 \n",
            "step  725/1000: dist 0.31 loss 0.35 \n",
            "step  726/1000: dist 0.31 loss 0.35 \n",
            "step  727/1000: dist 0.31 loss 0.33 \n",
            "step  728/1000: dist 0.31 loss 0.32 \n",
            "step  729/1000: dist 0.31 loss 0.34 \n",
            "step  730/1000: dist 0.31 loss 0.35 \n",
            "step  731/1000: dist 0.31 loss 0.35 \n",
            "step  732/1000: dist 0.31 loss 0.35 \n",
            "step  733/1000: dist 0.31 loss 0.39 \n",
            "step  734/1000: dist 0.31 loss 0.44 \n",
            "step  735/1000: dist 0.31 loss 0.52 \n",
            "step  736/1000: dist 0.31 loss 0.66 \n",
            "step  737/1000: dist 0.31 loss 0.93 \n",
            "step  738/1000: dist 0.31 loss 1.42 \n",
            "step  739/1000: dist 0.31 loss 2.28 \n",
            "step  740/1000: dist 0.31 loss 3.78 \n",
            "step  741/1000: dist 0.31 loss 6.26 \n",
            "step  742/1000: dist 0.31 loss 10.02\n",
            "step  743/1000: dist 0.31 loss 14.53\n",
            "step  744/1000: dist 0.31 loss 17.91\n",
            "step  745/1000: dist 0.31 loss 16.53\n",
            "step  746/1000: dist 0.31 loss 9.64 \n",
            "step  747/1000: dist 0.31 loss 2.16 \n",
            "step  748/1000: dist 0.31 loss 0.65 \n",
            "step  749/1000: dist 0.31 loss 4.86 \n",
            "step  750/1000: dist 0.31 loss 8.31 \n",
            "step  751/1000: dist 0.31 loss 6.31 \n",
            "step  752/1000: dist 0.31 loss 1.65 \n",
            "step  753/1000: dist 0.31 loss 0.58 \n",
            "step  754/1000: dist 0.31 loss 3.46 \n",
            "step  755/1000: dist 0.31 loss 5.03 \n",
            "step  756/1000: dist 0.31 loss 2.93 \n",
            "step  757/1000: dist 0.31 loss 0.93 \n",
            "step  758/1000: dist 0.31 loss 2.40 \n",
            "step  759/1000: dist 0.31 loss 5.26 \n",
            "step  760/1000: dist 0.31 loss 6.74 \n",
            "step  761/1000: dist 0.31 loss 8.93 \n",
            "step  762/1000: dist 0.31 loss 15.17\n",
            "step  763/1000: dist 0.31 loss 23.66\n",
            "step  764/1000: dist 0.31 loss 25.86\n",
            "step  765/1000: dist 0.31 loss 16.88\n",
            "step  766/1000: dist 0.31 loss 4.03 \n",
            "step  767/1000: dist 0.31 loss 1.88 \n",
            "step  768/1000: dist 0.31 loss 9.27 \n",
            "step  769/1000: dist 0.31 loss 11.42\n",
            "step  770/1000: dist 0.31 loss 4.38 \n",
            "step  771/1000: dist 0.31 loss 1.06 \n",
            "step  772/1000: dist 0.31 loss 5.86 \n",
            "step  773/1000: dist 0.31 loss 6.65 \n",
            "step  774/1000: dist 0.31 loss 1.53 \n",
            "step  775/1000: dist 0.31 loss 1.70 \n",
            "step  776/1000: dist 0.31 loss 5.06 \n",
            "step  777/1000: dist 0.31 loss 2.66 \n",
            "step  778/1000: dist 0.31 loss 0.44 \n",
            "step  779/1000: dist 0.31 loss 3.01 \n",
            "step  780/1000: dist 0.31 loss 2.81 \n",
            "step  781/1000: dist 0.31 loss 0.38 \n",
            "step  782/1000: dist 0.31 loss 1.67 \n",
            "step  783/1000: dist 0.31 loss 2.36 \n",
            "step  784/1000: dist 0.31 loss 0.54 \n",
            "step  785/1000: dist 0.31 loss 1.04 \n",
            "step  786/1000: dist 0.31 loss 1.77 \n",
            "step  787/1000: dist 0.31 loss 0.58 \n",
            "step  788/1000: dist 0.31 loss 0.81 \n",
            "step  789/1000: dist 0.31 loss 1.30 \n",
            "step  790/1000: dist 0.31 loss 0.49 \n",
            "step  791/1000: dist 0.31 loss 0.73 \n",
            "step  792/1000: dist 0.31 loss 0.98 \n",
            "step  793/1000: dist 0.31 loss 0.38 \n",
            "step  794/1000: dist 0.31 loss 0.68 \n",
            "step  795/1000: dist 0.31 loss 0.77 \n",
            "step  796/1000: dist 0.31 loss 0.32 \n",
            "step  797/1000: dist 0.31 loss 0.64 \n",
            "step  798/1000: dist 0.31 loss 0.60 \n",
            "step  799/1000: dist 0.31 loss 0.31 \n",
            "step  800/1000: dist 0.31 loss 0.58 \n",
            "step  801/1000: dist 0.31 loss 0.47 \n",
            "step  802/1000: dist 0.31 loss 0.33 \n",
            "step  803/1000: dist 0.30 loss 0.52 \n",
            "step  804/1000: dist 0.30 loss 0.37 \n",
            "step  805/1000: dist 0.30 loss 0.37 \n",
            "step  806/1000: dist 0.30 loss 0.45 \n",
            "step  807/1000: dist 0.30 loss 0.32 \n",
            "step  808/1000: dist 0.30 loss 0.39 \n",
            "step  809/1000: dist 0.30 loss 0.38 \n",
            "step  810/1000: dist 0.30 loss 0.32 \n",
            "step  811/1000: dist 0.30 loss 0.39 \n",
            "step  812/1000: dist 0.30 loss 0.32 \n",
            "step  813/1000: dist 0.30 loss 0.34 \n",
            "step  814/1000: dist 0.30 loss 0.36 \n",
            "step  815/1000: dist 0.30 loss 0.30 \n",
            "step  816/1000: dist 0.30 loss 0.35 \n",
            "step  817/1000: dist 0.30 loss 0.33 \n",
            "step  818/1000: dist 0.30 loss 0.31 \n",
            "step  819/1000: dist 0.30 loss 0.34 \n",
            "step  820/1000: dist 0.30 loss 0.31 \n",
            "step  821/1000: dist 0.30 loss 0.33 \n",
            "step  822/1000: dist 0.30 loss 0.32 \n",
            "step  823/1000: dist 0.30 loss 0.31 \n",
            "step  824/1000: dist 0.30 loss 0.32 \n",
            "step  825/1000: dist 0.30 loss 0.31 \n",
            "step  826/1000: dist 0.30 loss 0.32 \n",
            "step  827/1000: dist 0.30 loss 0.31 \n",
            "step  828/1000: dist 0.30 loss 0.31 \n",
            "step  829/1000: dist 0.30 loss 0.31 \n",
            "step  830/1000: dist 0.30 loss 0.30 \n",
            "step  831/1000: dist 0.30 loss 0.31 \n",
            "step  832/1000: dist 0.30 loss 0.30 \n",
            "step  833/1000: dist 0.30 loss 0.31 \n",
            "step  834/1000: dist 0.30 loss 0.31 \n",
            "step  835/1000: dist 0.30 loss 0.30 \n",
            "step  836/1000: dist 0.30 loss 0.31 \n",
            "step  837/1000: dist 0.30 loss 0.30 \n",
            "step  838/1000: dist 0.30 loss 0.31 \n",
            "step  839/1000: dist 0.30 loss 0.30 \n",
            "step  840/1000: dist 0.30 loss 0.31 \n",
            "step  841/1000: dist 0.30 loss 0.30 \n",
            "step  842/1000: dist 0.30 loss 0.30 \n",
            "step  843/1000: dist 0.30 loss 0.30 \n",
            "step  844/1000: dist 0.30 loss 0.30 \n",
            "step  845/1000: dist 0.30 loss 0.30 \n",
            "step  846/1000: dist 0.30 loss 0.30 \n",
            "step  847/1000: dist 0.30 loss 0.30 \n",
            "step  848/1000: dist 0.30 loss 0.30 \n",
            "step  849/1000: dist 0.30 loss 0.30 \n",
            "step  850/1000: dist 0.30 loss 0.30 \n",
            "step  851/1000: dist 0.30 loss 0.30 \n",
            "step  852/1000: dist 0.30 loss 0.30 \n",
            "step  853/1000: dist 0.30 loss 0.30 \n",
            "step  854/1000: dist 0.30 loss 0.30 \n",
            "step  855/1000: dist 0.30 loss 0.30 \n",
            "step  856/1000: dist 0.30 loss 0.30 \n",
            "step  857/1000: dist 0.30 loss 0.30 \n",
            "step  858/1000: dist 0.30 loss 0.30 \n",
            "step  859/1000: dist 0.30 loss 0.30 \n",
            "step  860/1000: dist 0.30 loss 0.30 \n",
            "step  861/1000: dist 0.30 loss 0.30 \n",
            "step  862/1000: dist 0.30 loss 0.30 \n",
            "step  863/1000: dist 0.30 loss 0.30 \n",
            "step  864/1000: dist 0.30 loss 0.30 \n",
            "step  865/1000: dist 0.30 loss 0.30 \n",
            "step  866/1000: dist 0.30 loss 0.30 \n",
            "step  867/1000: dist 0.30 loss 0.30 \n",
            "step  868/1000: dist 0.30 loss 0.30 \n",
            "step  869/1000: dist 0.30 loss 0.30 \n",
            "step  870/1000: dist 0.30 loss 0.30 \n",
            "step  871/1000: dist 0.30 loss 0.30 \n",
            "step  872/1000: dist 0.30 loss 0.30 \n",
            "step  873/1000: dist 0.30 loss 0.30 \n",
            "step  874/1000: dist 0.30 loss 0.30 \n",
            "step  875/1000: dist 0.30 loss 0.30 \n",
            "step  876/1000: dist 0.30 loss 0.30 \n",
            "step  877/1000: dist 0.30 loss 0.30 \n",
            "step  878/1000: dist 0.30 loss 0.30 \n",
            "step  879/1000: dist 0.30 loss 0.30 \n",
            "step  880/1000: dist 0.30 loss 0.30 \n",
            "step  881/1000: dist 0.30 loss 0.30 \n",
            "step  882/1000: dist 0.30 loss 0.30 \n",
            "step  883/1000: dist 0.30 loss 0.30 \n",
            "step  884/1000: dist 0.30 loss 0.30 \n",
            "step  885/1000: dist 0.30 loss 0.30 \n",
            "step  886/1000: dist 0.30 loss 0.30 \n",
            "step  887/1000: dist 0.30 loss 0.30 \n",
            "step  888/1000: dist 0.30 loss 0.30 \n",
            "step  889/1000: dist 0.30 loss 0.30 \n",
            "step  890/1000: dist 0.30 loss 0.30 \n",
            "step  891/1000: dist 0.30 loss 0.30 \n",
            "step  892/1000: dist 0.30 loss 0.30 \n",
            "step  893/1000: dist 0.30 loss 0.30 \n",
            "step  894/1000: dist 0.30 loss 0.30 \n",
            "step  895/1000: dist 0.30 loss 0.30 \n",
            "step  896/1000: dist 0.30 loss 0.30 \n",
            "step  897/1000: dist 0.30 loss 0.30 \n",
            "step  898/1000: dist 0.30 loss 0.30 \n",
            "step  899/1000: dist 0.30 loss 0.30 \n",
            "step  900/1000: dist 0.30 loss 0.30 \n",
            "step  901/1000: dist 0.30 loss 0.30 \n",
            "step  902/1000: dist 0.30 loss 0.30 \n",
            "step  903/1000: dist 0.30 loss 0.30 \n",
            "step  904/1000: dist 0.30 loss 0.30 \n",
            "step  905/1000: dist 0.30 loss 0.30 \n",
            "step  906/1000: dist 0.30 loss 0.30 \n",
            "step  907/1000: dist 0.30 loss 0.30 \n",
            "step  908/1000: dist 0.30 loss 0.30 \n",
            "step  909/1000: dist 0.30 loss 0.30 \n",
            "step  910/1000: dist 0.30 loss 0.30 \n",
            "step  911/1000: dist 0.30 loss 0.30 \n",
            "step  912/1000: dist 0.30 loss 0.30 \n",
            "step  913/1000: dist 0.30 loss 0.30 \n",
            "step  914/1000: dist 0.30 loss 0.30 \n",
            "step  915/1000: dist 0.30 loss 0.30 \n",
            "step  916/1000: dist 0.30 loss 0.30 \n",
            "step  917/1000: dist 0.30 loss 0.30 \n",
            "step  918/1000: dist 0.30 loss 0.30 \n",
            "step  919/1000: dist 0.30 loss 0.30 \n",
            "step  920/1000: dist 0.30 loss 0.30 \n",
            "step  921/1000: dist 0.30 loss 0.30 \n",
            "step  922/1000: dist 0.30 loss 0.30 \n",
            "step  923/1000: dist 0.30 loss 0.30 \n",
            "step  924/1000: dist 0.30 loss 0.30 \n",
            "step  925/1000: dist 0.30 loss 0.30 \n",
            "step  926/1000: dist 0.30 loss 0.30 \n",
            "step  927/1000: dist 0.30 loss 0.30 \n",
            "step  928/1000: dist 0.30 loss 0.30 \n",
            "step  929/1000: dist 0.30 loss 0.30 \n",
            "step  930/1000: dist 0.30 loss 0.30 \n",
            "step  931/1000: dist 0.30 loss 0.30 \n",
            "step  932/1000: dist 0.30 loss 0.30 \n",
            "step  933/1000: dist 0.30 loss 0.30 \n",
            "step  934/1000: dist 0.30 loss 0.30 \n",
            "step  935/1000: dist 0.30 loss 0.30 \n",
            "step  936/1000: dist 0.30 loss 0.30 \n",
            "step  937/1000: dist 0.30 loss 0.30 \n",
            "step  938/1000: dist 0.30 loss 0.30 \n",
            "step  939/1000: dist 0.30 loss 0.30 \n",
            "step  940/1000: dist 0.30 loss 0.30 \n",
            "step  941/1000: dist 0.30 loss 0.30 \n",
            "step  942/1000: dist 0.30 loss 0.30 \n",
            "step  943/1000: dist 0.30 loss 0.30 \n",
            "step  944/1000: dist 0.30 loss 0.30 \n",
            "step  945/1000: dist 0.30 loss 0.30 \n",
            "step  946/1000: dist 0.30 loss 0.30 \n",
            "step  947/1000: dist 0.30 loss 0.30 \n",
            "step  948/1000: dist 0.30 loss 0.30 \n",
            "step  949/1000: dist 0.30 loss 0.30 \n",
            "step  950/1000: dist 0.30 loss 0.30 \n",
            "step  951/1000: dist 0.30 loss 0.30 \n",
            "step  952/1000: dist 0.30 loss 0.30 \n",
            "step  953/1000: dist 0.30 loss 0.30 \n",
            "step  954/1000: dist 0.30 loss 0.30 \n",
            "step  955/1000: dist 0.30 loss 0.30 \n",
            "step  956/1000: dist 0.30 loss 0.30 \n",
            "step  957/1000: dist 0.30 loss 0.30 \n",
            "step  958/1000: dist 0.30 loss 0.30 \n",
            "step  959/1000: dist 0.30 loss 0.30 \n",
            "step  960/1000: dist 0.30 loss 0.30 \n",
            "step  961/1000: dist 0.30 loss 0.30 \n",
            "step  962/1000: dist 0.30 loss 0.30 \n",
            "step  963/1000: dist 0.30 loss 0.30 \n",
            "step  964/1000: dist 0.30 loss 0.30 \n",
            "step  965/1000: dist 0.30 loss 0.30 \n",
            "step  966/1000: dist 0.30 loss 0.30 \n",
            "step  967/1000: dist 0.30 loss 0.30 \n",
            "step  968/1000: dist 0.30 loss 0.30 \n",
            "step  969/1000: dist 0.30 loss 0.30 \n",
            "step  970/1000: dist 0.30 loss 0.30 \n",
            "step  971/1000: dist 0.30 loss 0.30 \n",
            "step  972/1000: dist 0.30 loss 0.30 \n",
            "step  973/1000: dist 0.30 loss 0.30 \n",
            "step  974/1000: dist 0.30 loss 0.30 \n",
            "step  975/1000: dist 0.30 loss 0.30 \n",
            "step  976/1000: dist 0.30 loss 0.30 \n",
            "step  977/1000: dist 0.30 loss 0.30 \n",
            "step  978/1000: dist 0.30 loss 0.30 \n",
            "step  979/1000: dist 0.30 loss 0.30 \n",
            "step  980/1000: dist 0.30 loss 0.30 \n",
            "step  981/1000: dist 0.30 loss 0.30 \n",
            "step  982/1000: dist 0.30 loss 0.30 \n",
            "step  983/1000: dist 0.30 loss 0.30 \n",
            "step  984/1000: dist 0.30 loss 0.30 \n",
            "step  985/1000: dist 0.30 loss 0.30 \n",
            "step  986/1000: dist 0.30 loss 0.30 \n",
            "step  987/1000: dist 0.30 loss 0.30 \n",
            "step  988/1000: dist 0.30 loss 0.30 \n",
            "step  989/1000: dist 0.30 loss 0.30 \n",
            "step  990/1000: dist 0.30 loss 0.30 \n",
            "step  991/1000: dist 0.30 loss 0.30 \n",
            "step  992/1000: dist 0.30 loss 0.30 \n",
            "step  993/1000: dist 0.30 loss 0.30 \n",
            "step  994/1000: dist 0.30 loss 0.30 \n",
            "step  995/1000: dist 0.30 loss 0.30 \n",
            "step  996/1000: dist 0.30 loss 0.30 \n",
            "step  997/1000: dist 0.30 loss 0.30 \n",
            "step  998/1000: dist 0.30 loss 0.30 \n",
            "step  999/1000: dist 0.30 loss 0.30 \n",
            "step 1000/1000: dist 0.30 loss 0.30 \n",
            "Elapsed: 110.8 s\n"
          ]
        }
      ],
      "source": [
        "# random한 seed 설정\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Load networks.\n",
        "print('Loading networks')\n",
        "device = torch.device('cuda')\n",
        "with open('/content/drive/MyDrive/stylegan2-ada-pytorch/network-snapshot-003600.pkl', 'rb') as f:\n",
        "    G = legacy.load_network_pkl(f)['G'].requires_grad_(False).to(device)\n",
        "# Load target image.\n",
        "#초음파 이미지의 경로를 설정\n",
        "target_fname = '/content/kongdak3.jpg'\n",
        "\n",
        "target_pil = PIL.Image.open(target_fname).convert('RGB')\n",
        "# plt.imshow(target_pil)\n",
        "w, h = target_pil.size\n",
        "s = min(w, h)\n",
        "target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
        "target_pil = target_pil.resize((G.img_resolution, G.img_resolution), PIL.Image.LANCZOS)\n",
        "target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
        "\n",
        "# Optimize projection.-> 최적의 projection을 위한 학습을 진행\n",
        "start_time = perf_counter()\n",
        "projected_w_steps = project(\n",
        "    G,\n",
        "    target=torch.tensor(target_uint8.transpose([2, 0, 1]), device=device), # pylint: disable=not-callable\n",
        "    num_steps=1000,\n",
        "    device=device,\n",
        "    verbose=True\n",
        ")\n",
        "print (f'Elapsed: {(perf_counter()-start_time):.1f} s')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(outdir, exist_ok=True)"
      ],
      "metadata": {
        "id": "a2x0GsK90vrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#저장할 번호\n",
        "imgnum='kongdak3_weight3600_0823'"
      ],
      "metadata": {
        "id": "mgj5ZRuOHG5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8MJxXWsY-q9"
      },
      "source": [
        "만드는 과정이 비디오로 저장됨+projected_w가 out 폴더에 저장됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDehhA0yX6qD"
      },
      "source": [
        "style mixing 코드로 초음파이미지로부터 z를 얻고 신생아이미지로 학습한 가중치를 더함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Fj67A_dDYFCW"
      },
      "outputs": [],
      "source": [
        "# @title style mixing -1\n",
        "def num_range(s: str) -> List[int]:\n",
        "    '''Accept either a comma separated list of numbers 'a,b,c' or a range 'a-c' and return as a list of ints.'''\n",
        "\n",
        "    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n",
        "    m = range_re.match(s)\n",
        "    if m:\n",
        "        return list(range(int(m.group(1)), int(m.group(2))+1))\n",
        "    vals = s.split(',')\n",
        "    return [int(x) for x in vals]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UGzgcVx0YIxk"
      },
      "outputs": [],
      "source": [
        "# @title style mixing -2\n",
        "def generate_style_mix(\n",
        "    network_pkl: str,\n",
        "    row_seeds: List[int],\n",
        "    col_seeds: List[int],\n",
        "    col_styles: List[int],\n",
        "    truncation_psi: float,\n",
        "    noise_mode: str,\n",
        "    outdir: str\n",
        "):\n",
        "    print('Loading networks')\n",
        "    device = torch.device('cuda')\n",
        "    with open('/content/drive/MyDrive/stylegan3/training-runs/00006-stylegan3-t-dataset-gpus1-batch4-gamma8.2/network-snapshot-000001.pkl', 'rb') as f:\n",
        "        G = pickle.load(f)['G'].cuda()\n",
        "    #출력물이 위치할 directory 설정\n",
        "    outdir = '/content/drive/MyDrive/stylegan3/stylegan2-ada-pytorch/out'\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    print('Generating W vectors...')\n",
        "    all_seeds = list(set(row_seeds + col_seeds))\n",
        "    all_z = np.stack([np.random.RandomState(seed).randn(G.z_dim) for seed in all_seeds])\n",
        "    all_w = G.mapping(torch.from_numpy(all_z).to(device), None)\n",
        "    w_avg = G.mapping.w_avg\n",
        "    all_w = w_avg + (all_w - w_avg) * truncation_psi\n",
        "    w_dict = {seed: w for seed, w in zip(all_seeds, list(all_w))}\n",
        "\n",
        "    print('Generating images...')\n",
        "    all_images = G.synthesis(all_w, noise_mode=noise_mode)\n",
        "    all_images = (all_images.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
        "    image_dict = {(seed, seed): image for seed, image in zip(all_seeds, list(all_images))}\n",
        "\n",
        "    print('Generating style-mixed images...')\n",
        "    for row_seed in row_seeds:\n",
        "        for col_seed in col_seeds:\n",
        "            w = w_dict[row_seed].clone()\n",
        "            w[col_styles] = w_dict[col_seed][col_styles]\n",
        "            image = G.synthesis(w[np.newaxis], noise_mode=noise_mode)\n",
        "            image = (image.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "            image_dict[(row_seed, col_seed)] = image[0].cpu().numpy()\n",
        "\n",
        "    print('Saving images...')\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    for (row_seed, col_seed), image in image_dict.items():\n",
        "        PIL.Image.fromarray(image, 'RGB').save(f'{outdir}/{row_seed}-{col_seed}.png')\n",
        "\n",
        "    print('Saving image grid...')\n",
        "    W = G.img_resolution\n",
        "    H = G.img_resolution\n",
        "    canvas = PIL.Image.new('RGB', (W * (len(col_seeds) + 1), H * (len(row_seeds) + 1)), 'black')\n",
        "    for row_idx, row_seed in enumerate([0] + row_seeds):\n",
        "        for col_idx, col_seed in enumerate([0] + col_seeds):\n",
        "            if row_idx == 0 and col_idx == 0:\n",
        "                continue\n",
        "            key = (row_seed, col_seed)\n",
        "            if row_idx == 0:\n",
        "                key = (col_seed, col_seed)\n",
        "            if col_idx == 0:\n",
        "                key = (row_seed, row_seed)\n",
        "            canvas.paste(PIL.Image.fromarray(image_dict[key], 'RGB'), (W * col_idx, H * row_idx))\n",
        "    canvas.save(f'{outdir}/grid.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MTHSU1nRaN3P"
      },
      "outputs": [],
      "source": [
        "# @title style mixing -3\n",
        "generate_style_mix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J47ssAzaBLMe"
      },
      "source": [
        "# 초음파 이미지로부터 생성된 z벡터를 통해 img를 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVOVpreGY6hE"
      },
      "outputs": [],
      "source": [
        "# # # Save final projected frame and W vector.target_pil.save(f'{outdir}/target.png')\n",
        "# projected_w = projected_w_steps[-1]\n",
        "# synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
        "# synth_image = (synth_image + 1) * (255/2)\n",
        "# synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "# PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/proj.png')\n",
        "# np.savez(f'{outdir}/projected_w3.npz', w=projected_w.unsqueeze(0).cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save final projected frame\n",
        "projected_w = projected_w_steps[-1]\n",
        "synth_image = G(projected_w.cuda(), None)\n",
        "synth_image = (synth_image + 1) * (255/2) #N(0,1) -> RGB값으로\n",
        "synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/targe_{imgnum}.png')"
      ],
      "metadata": {
        "id": "Wh9LoYpM756s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #압축된 npz 파일을 볼 수 있는 배열 형태로 변경\n",
        "# a=np.load('/content/drive/MyDrive/stylegan2-ada-pytorch/training-runs/projected_w3.npz')\n",
        "# w_vector = a['w']"
      ],
      "metadata": {
        "id": "M25y5_hETjVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video = imageio.get_writer(f'{outdir}/proj_{imgnum}.mp4', mode='I', fps=100, codec='libx264', bitrate='16M')\n",
        "print (f'Saving optimization progress video \"{outdir}/proj_{imgnum}.mp4\"')\n",
        "for projected_w in projected_w_steps:\n",
        "    synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\n",
        "    synth_image = G(projected_w.cuda(), None)\n",
        "    synth_image = (synth_image + 1) * (255/2) #N(0,1) -> RGB값으로\n",
        "    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy() #이미지화\n",
        "    video.append_data(np.concatenate([target_uint8, synth_image], axis=1))\n",
        "video.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joyXqpiKw852",
        "outputId": "7f71caf3-a7ba-4be5-b808-58bd314dd21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving optimization progress video \"/content/drive/MyDrive/stylegan2-ada-pytorch/training-runs2/proj_test1_0822.mp4\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################################################################################################"
      ],
      "metadata": {
        "id": "pB5msGMcHN-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkQ1p--hcOiN"
      },
      "outputs": [],
      "source": [
        "# z = projected_w[-1]\n",
        "tensorz = torch.Tensor(z)\n",
        "tensorz_with_batch = tensorz.unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW_318s3BL5G"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/stylegan2-ada-pytorch/network-snapshot-003600.pkl', 'rb') as f:\n",
        "    G = pickle.load(f)['G_ema'].cuda()  # torch.nn.Module\n",
        "c = None                                # class labels (not used in this example)\n",
        "img = G(tensorz_with_batch.cuda(), c)                           # NCHW, float32, dynamic range [-1, +1], no truncation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synth_image =img.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/target.png')"
      ],
      "metadata": {
        "id": "XwkIXW8c6cXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA0xKJWjE0r0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7450ab80-9e01-4a6f-94de-8ee340ada0d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "img1=img.permute(0,2,3,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgrrdX4zGl2n"
      },
      "outputs": [],
      "source": [
        "img1 = img1.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0ct7uOPFNrN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "img2 = img1.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(img1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW4tClAg1Ry1",
        "outputId": "0ab75cc3-01e0-4b4d-e66d-4b9469869318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 512, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "43xq6c22VD20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMR6tJFkGxr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a852895a-7508-4457-c0e4-59aa3dee7e9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, 512, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "img2.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# succ, enc_image = cv2.imencode('.jpg', img2)\n",
        "# image_bytes = enc_image.tobytes()\n",
        "# plt.imshow(\n",
        "#     cv2.cvtColor(image_bytes, cv2.COLOR_BGR2RGB)\n",
        "#     )\n",
        "# plt.show()​"
      ],
      "metadata": {
        "id": "QAS04XIVRm9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img2 = (img2 +1)*(255/2)"
      ],
      "metadata": {
        "id": "2Kk4k2jZ2HYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.savefig('test_graph.png',\n",
        "            facecolor='#eeeeee',\n",
        "            edgecolor='black',\n",
        "            format='png', dpi=200)"
      ],
      "metadata": {
        "id": "JEpfI9ZC5cZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeAQ5TfkcHsv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "195a1fe8-a91b-4cd8-808b-530e37845762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x78e36023c280>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(img2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3A51pc7pRkDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "17e-VilB894ea_yKgzruC6jRBOGeHbAn-",
      "authorship_tag": "ABX9TyNVOxRK99wXo0PxQBcn1edh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}